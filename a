Je m'excuse pour la confusion. J'ai réalisé que `model_class` est défini localement dans le bloc `if` et n'est pas accessible pour la création du pipeline hors de la fonction `objective`. Nous devons ajuster la logique pour que `model_class` soit correctement défini et utilisé.

Voici comment vous pouvez résoudre le problème :

```python
import optuna
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.model_selection import cross_val_score, train_test_split
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Supposons que X et y sont déjà définis
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

def optimize_hyperparameters(model_name, X_train, y_train, n_trials=10):
    def objective(trial):
        if model_name == 'RandomForest':
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
            max_depth = trial.suggest_int('max_depth', 5, 20)
            model = RandomForestClassifier(random_state=42, n_estimators=n_estimators, max_depth=max_depth)

        elif model_name == 'AdaBoost':
            n_estimators = trial.suggest_int('n_estimators', 50, 300)
            learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0)
            model = AdaBoostClassifier(random_state=42, n_estimators=n_estimators, learning_rate=learning_rate)

        elif model_name == 'KNN':
            n_neighbors = trial.suggest_int('n_neighbors', 3, 30)
            model = KNeighborsClassifier(n_neighbors=n_neighbors)

        elif model_name == 'SVM':
            C = trial.suggest_float('C', 0.01, 10.0)
            model = SVC(random_state=42, C=C)

        elif model_name == 'XGB':
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
            max_depth = trial.suggest_int('max_depth', 3, 20)
            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)
            model = XGBClassifier(random_state=42, n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)

        elif model_name == 'LGBM':
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
            num_leaves = trial.suggest_int('num_leaves', 31, 200)
            model = LGBMClassifier(random_state=42, n_estimators=n_estimators, num_leaves=num_leaves)

        elif model_name == 'Logistic':
            C = trial.suggest_float('C', 0.01, 10.0)
            model = LogisticRegression(random_state=42, C=C)

        else:
            raise NotImplementedError()

        preprocessor = SelectKBest(f_classif, k=7)
        pipeline = make_pipeline(preprocessor, model)

        return cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1', n_jobs=-1).mean()

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)

    # Find the best model and its parameters after the study
    best_trial = study.best_trial
    if model_name == 'RandomForest':
        model = RandomForestClassifier(**best_trial.params, random_state=42)
    elif model_name == 'AdaBoost':
        model = AdaBoostClassifier(**best_trial.params, random_state=42)
    elif model_name == 'KNN':
        model = KNeighborsClassifier(**best_trial.params)
    elif model_name == 'SVM':
        model = SVC(**best_trial.params, random_state=42)
    elif model_name == 'XGB':
        model = XGBClassifier(**best_trial.params, random_state=42)
    elif model_name == 'LGBM':
        model = LGBMClassifier(**best_trial.params, random_state=42)
    elif model_name == 'Logistic':
        model = LogisticRegression(**best_trial.params, random_state=42)

    preprocessor = SelectKBest(f_classif, k=7)
    best_pipeline = make_pipeline(preprocessor, model)

    return best_pipeline

model_names = ['RandomForest', 'AdaBoost', 'KNN', 'SVM', 'XGB', 'LGBM', '

Logistic']

def evaluate_models(model_names, X_train, y_train, X_val, y_val, n_trials=10):
    results = {}
    for model_name in model_names:
        print(f"Optimizing and evaluating {model_name}...")
        best_pipeline = optimize_hyperparameters(model_name, X_train, y_train, n_trials=n_trials)
        best_pipeline.fit(X_train, y_train)
        y_pred = best_pipeline.predict(X_val)
        
        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='binary')  # Change 'binary' to 'macro' or 'weighted' for multiclass
        recall = recall_score(y_val, y_pred, average='binary')

        results[model_name] = {'accuracy': accuracy, 'f1': f1, 'recall': recall}

    return results

# Evaluate the models
evaluation_results = evaluate_models(model_names, X_train, y_train, X_val, y_val)

# Display the results
print("Model evaluation results:")
for model_name, metrics in evaluation_results.items():
    print(f"\n{model_name}:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name}: {metric_value:.2f}")
```

Ce code devrait maintenant fonctionner correctement sans l'erreur `name 'model_class' is not defined`, car nous redéfinissons le modèle après avoir obtenu les meilleurs paramètres à partir de l'étude Optuna.
