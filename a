import optuna
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.model_selection import cross_val_score, train_test_split
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Supposons que X et y sont déjà définis
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

def optimize_hyperparameters(model_name, X_train, y_train, n_trials=10):
    def objective(trial):
        preprocessor = SelectKBest(f_classif, k=7)

        if model_name == 'RandomForest':
            model_class = RandomForestClassifier
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
            max_depth = trial.suggest_int('max_depth', 5, 20)
            model = model_class(random_state=42, n_estimators=n_estimators, max_depth=max_depth)

        elif model_name == 'AdaBoost':
            model_class = AdaBoostClassifier
            n_estimators = trial.suggest_int('n_estimators', 50, 300)
            learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0)
            model = model_class(random_state=42, n_estimators=n_estimators, learning_rate=learning_rate)

        elif model_name == 'KNN':
            model_class = KNeighborsClassifier
            n_neighbors = trial.suggest_int('n_neighbors', 3, 30)
            model = model_class(n_neighbors=n_neighbors)

        elif model_name == 'SVM':
            model_class = SVC
            C = trial.suggest_float('C', 0.01, 10.0)
            model = model_class(random_state=42, C=C)

        elif model_name == 'XGB':
            model_class = XGBClassifier
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
            max_depth = trial.suggest_int('max_depth', 3, 20)
            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)
            model = model_class(random_state=42, n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)

        elif model_name == 'LGBM':
            model_class = LGBMClassifier
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
            num_leaves = trial.suggest_int('num_leaves', 31, 200)
            model = model_class(random_state=42, n_estimators=n_estimators, num_leaves=num_leaves)

        elif model_name == 'Logistic':
            model_class = LogisticRegression
            C = trial.suggest_float('C', 0.01, 10.0)
            model = model_class(random_state=42, C=C)

        else:
            raise NotImplementedError()

        pipeline = make_pipeline(preprocessor, model)
        score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1', n_jobs=-1).mean()
        return score

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)
    
    best_model = model_class(**study.best_params)
    best_pipeline = make_pipeline(preprocessor, best_model)

    return best_pipeline

model_names = ['RandomForest', 'AdaBoost', 'KNN', 'SVM', 'XGB', 'LGBM', 'Logistic']

def evaluate_models(model_names, X_train, y_train, X_val, y_val, n_trials=10):
    results = {}
    for model_name in model_names:
        print(f"Optimizing and evaluating {model_name}...")
        best_pipeline = optimize_hyperparameters(model_name, X_train, y_train, n_trials=n_trials)
        best_pipeline.fit(X_train, y_train)
        y_pred = best_pipeline.predict(X_val)
        
        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='binary')  # Change 'binary' to 'macro' or 'weighted' for multiclass
        recall = recall_score(y_val, y_pred, average='binary')

        results[model_name] = {'accuracy': accuracy, 'f1': f1, 'recall': recall}

    return results

# Evaluating the models
evaluation_results = evaluate_models(model_names, X_train, y_train, X_val, y_val)

# Displaying the results
print("Model evaluation results:")
for model_name, metrics in evaluation_results.items():
    print(f"\n{model_name}:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name}: {metric_value:.2f}")
